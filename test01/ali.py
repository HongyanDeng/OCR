import os
import paddle
from paddleocr import PPStructureV3
from PIL import Image
import json
import pandas as pd
import numpy as np
import time
import psutil
from typing import List, Union


class TableOCRProcessor:
    def __init__(self):
        """åˆå§‹åŒ–OCRå¤„ç†å™¨"""
        self._print_system_info()
        self._init_paddle()
        self.model = self._init_model()

    def _print_system_info(self):
        """æ‰“å°ç³»ç»Ÿå’Œç¡¬ä»¶ä¿¡æ¯"""
        print("\nğŸ–¥ï¸ ç³»ç»Ÿä¿¡æ¯:")
        print(f"CPUæ ¸å¿ƒæ•°: {os.cpu_count()}")
        print(f"ç³»ç»Ÿå†…å­˜: {psutil.virtual_memory().total / 1024 ** 3:.2f} GB")
        if paddle.is_compiled_with_cuda():
            try:
                props = paddle.device.cuda.get_device_properties()
                print(f"GPUå‹å·: {props.name}")
                print(f"GPUæ˜¾å­˜: {props.total_memory / 1024 ** 3:.2f} GB")
            except Exception as e:
                print(f"è·å–GPUä¿¡æ¯å¤±è´¥: {str(e)}")

    def _init_paddle(self):
        """é…ç½®PaddlePaddleå‚æ•°"""
        if paddle.is_compiled_with_cuda():
            paddle.set_device('gpu')
            print("\nâœ… å·²å¯ç”¨GPUåŠ é€Ÿ")
            paddle.set_flags({
                'FLAGS_conv_workspace_size_limit': 128,
                'FLAGS_cudnn_exhaustive_search': 1,
                'FLAGS_allocator_strategy': 'auto_growth'
            })
        else:
            paddle.set_device('cpu')
            print("\nâš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUæ¨¡å¼")

    def _init_model(self):
        """åˆå§‹åŒ–PPStructureV3æ¨¡å‹"""
        print("\nğŸ”¥ æ¨¡å‹åˆå§‹åŒ–ä¸­...")
        start_time = time.time()

        model = PPStructureV3()

        # å°å‹é¢„çƒ­
        try:
            dummy_img = np.zeros((100, 100, 3), dtype=np.uint8)
            model.predict(dummy_img)
        except Exception as e:
            print(f"é¢„çƒ­è­¦å‘Š: {str(e)}")

        print(f"æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
        return model

    def preprocess_image(self, image_path: str, max_size: int = 1200) -> Image.Image:
        """å›¾åƒé¢„å¤„ç†"""
        print(f"\nğŸ–¼ï¸ é¢„å¤„ç†å›¾ç‰‡: {os.path.basename(image_path)}")
        try:
            img = Image.open(image_path)
            if img.mode == 'RGBA':
                img = img.convert('RGB')
                print("  å·²è½¬æ¢RGBAä¸ºRGBæ ¼å¼")

            w, h = img.size
            if max(w, h) > max_size:
                scale = max_size / max(w, h)
                new_w, new_h = int(w * scale), int(h * scale)
                img = img.resize((new_w, new_h), Image.LANCZOS)
                print(f"  å›¾ç‰‡ä» ({w},{h}) ç¼©æ”¾è‡³ ({new_w},{new_h})")

            return img
        except Exception as e:
            print(f"âŒ å›¾ç‰‡é¢„å¤„ç†å¤±è´¥: {str(e)}")
            raise

    def process_single_image(self, image_path: str, output_dir: str = "output") -> dict:
        """å¤„ç†å•å¼ å›¾ç‰‡"""
        result_info = {
            "image": os.path.basename(image_path),
            "json_path": None,
            "csv_path": None,
            "success": False
        }

        os.makedirs(output_dir, exist_ok=True)
        base_name = os.path.splitext(os.path.basename(image_path))[0]

        try:
            # 1. é¢„å¤„ç†å›¾åƒ
            img = self.preprocess_image(image_path)
            temp_img_path = os.path.join(output_dir, f"temp_{base_name}.jpg")
            img.save(temp_img_path, quality=95, subsampling=0)

            # 2. æ‰§è¡ŒOCR
            print(f"ğŸ” æ­£åœ¨è¯†åˆ«: {base_name}")
            ocr_start = time.time()
            result = self.model.predict(temp_img_path)
            print(f"  OCRæ ¸å¿ƒè€—æ—¶: {time.time() - ocr_start:.2f}ç§’")

            # 3. ä¿å­˜JSONç»“æœ
            json_path = os.path.join(output_dir, f"{base_name}_result.json")
            with open(json_path, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2, default=str)
            result_info["json_path"] = json_path

            # 4. æå–è¡¨æ ¼æ•°æ®ï¼ˆä¿®å¤äº†ç±»å‹é”™è¯¯ï¼‰
            if result and isinstance(result, list):
                for page in result:
                    if 'overall_ocr_res' in page:
                        ocr_res = page['overall_ocr_res']
                        rec_texts = ocr_res.get('rec_texts', [])
                        dt_polys = ocr_res.get('dt_polys', [])

                        # ç¡®ä¿åæ ‡æ˜¯å¯ç”¨æ ¼å¼
                        if len(rec_texts) == len(dt_polys):
                            df = self._reconstruct_table(rec_texts, dt_polys)
                            csv_path = os.path.join(output_dir, f"{base_name}_table.csv")
                            df.to_csv(csv_path, index=False, encoding="utf-8-sig")
                            result_info["csv_path"] = csv_path
                            print(f"ğŸ“Š è¡¨æ ¼å·²ä¿å­˜: {csv_path}")
                            result_info["success"] = True

            return result_info

        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {base_name} - {str(e)}")
            return result_info
        finally:
            if 'temp_img_path' in locals() and os.path.exists(temp_img_path):
                os.remove(temp_img_path)
            paddle.device.cuda.empty_cache()

    def _reconstruct_table(self, rec_texts: List[str], dt_polys: List[Union[str, np.ndarray]]) -> pd.DataFrame:
        """é‡å»ºè¡¨æ ¼ï¼ˆå…¼å®¹å­—ç¬¦ä¸²å’Œnumpyæ•°ç»„è¾“å…¥ï¼‰"""
        boxes = []
        for poly in dt_polys:
            if isinstance(poly, np.ndarray):
                boxes.append(poly.tolist())
            elif isinstance(poly, str):
                boxes.append(self._parse_poly_str(poly))
            else:
                boxes.append(self._parse_poly_str(str(poly)))

        rows_indices = self._cluster_rows(boxes)
        top_lefts = np.array([box[0] for box in boxes])

        rows = []
        for row in rows_indices:
            row_sorted = sorted(row, key=lambda i: top_lefts[i, 0])
            rows.append([rec_texts[i] for i in row_sorted])

        max_cols = max(len(row) for row in rows)
        return pd.DataFrame([row + [''] * (max_cols - len(row)) for row in rows])

    @staticmethod
    def _parse_poly_str(poly_str: str) -> List[List[int]]:
        """è§£æåæ ‡å­—ç¬¦ä¸²"""
        cleaned = ''.join(c if c.isdigit() or c.isspace() else ' ' for c in poly_str)
        points = np.fromstring(cleaned, sep=' ', dtype=int).reshape(-1, 2)
        return points.tolist()

    @staticmethod
    def _cluster_rows(boxes: List[List[List[int]]], threshold: int = 15) -> List[np.ndarray]:
        """è¡Œèšç±»ç®—æ³•"""
        top_lefts = np.array([box[0] for box in boxes])
        y_coords = top_lefts[:, 1]
        sorted_idx = np.argsort(y_coords)
        diffs = np.diff(y_coords[sorted_idx])
        split_points = np.where(diffs > threshold)[0] + 1
        return np.split(sorted_idx, split_points)


def batch_process_images(image_paths: List[str], output_dir: str = "ocr_results"):
    """æ‰¹é‡å¤„ç†å›¾ç‰‡"""
    print("\n" + "=" * 50)
    print(f"ğŸ› ï¸ å¼€å§‹æ‰¹é‡å¤„ç† {len(image_paths)} å¼ å›¾ç‰‡")
    print("=" * 50)

    total_start = time.time()
    processor = TableOCRProcessor()

    results = []
    for img_path in image_paths:
        if not os.path.exists(img_path):
            print(f"\nâŒ æ–‡ä»¶ä¸å­˜åœ¨: {img_path}")
            continue

        print("\n" + "-" * 50)
        print(f"ğŸ“Œ æ­£åœ¨å¤„ç†: {os.path.basename(img_path)}")
        print("-" * 50)

        start_time = time.time()
        result = processor.process_single_image(img_path, output_dir)
        result["process_time"] = time.time() - start_time
        results.append(result)

        if result["success"]:
            print(f"âœ… å¤„ç†æˆåŠŸ (è€—æ—¶: {result['process_time']:.2f}ç§’)")
        else:
            print(f"âŒ å¤„ç†å¤±è´¥ (è€—æ—¶: {result['process_time']:.2f}ç§’)")

    # æ‰“å°æ±‡æ€»ä¿¡æ¯
    print("\n" + "=" * 50)
    print("ğŸ“Š æ‰¹é‡å¤„ç†ç»“æœæ±‡æ€»")
    print("=" * 50)
    success_count = sum(1 for r in results if r["success"])
    avg_time = sum(r["process_time"] for r in results) / len(results) if results else 0

    print(f"æˆåŠŸ: {success_count}/{len(image_paths)}")
    print(f"å¹³å‡è€—æ—¶: {avg_time:.2f}ç§’/å¼ ")
    print(f"æ€»è€—æ—¶: {time.time() - total_start:.2f}ç§’")

    # ä¿å­˜æ±‡æ€»æ—¥å¿—
    summary_path = os.path.join(output_dir, "processing_summary.json")
    with open(summary_path, "w", encoding="utf-8") as f:
        json.dump({
            "total_images": len(image_paths),
            "success_count": success_count,
            "average_time": avg_time,
            "details": results
        }, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ“ è¯¦ç»†æ—¥å¿—å·²ä¿å­˜è‡³: {summary_path}")


if __name__ == "__main__":
    # è®¾ç½®æ··åˆç²¾åº¦
    try:
        paddle.set_default_dtype('float16')
        print("\nğŸ”¼ å·²å¯ç”¨float16æ··åˆç²¾åº¦")
    except:
        print("\nğŸ”½ å½“å‰ç¯å¢ƒä¸æ”¯æŒfloat16ï¼Œä½¿ç”¨é»˜è®¤ç²¾åº¦")

    # è¦å¤„ç†çš„å›¾ç‰‡åˆ—è¡¨
    image_files = ["t.jpg", "wf.jpg"]

    # è¿‡æ»¤å‡ºå®é™…å­˜åœ¨çš„æ–‡ä»¶
    valid_images = [img for img in image_files if os.path.exists(img)]
    if not valid_images:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„å›¾ç‰‡æ–‡ä»¶")
    else:
        batch_process_images(valid_images)