# upload_to_milvus.py
import pandas as pd
import numpy as np
import os
from pymilvus import (
    connections,
    CollectionSchema,
    FieldSchema,
    DataType,
    utility,
    Collection
)
from pymilvus.bulk_writer import RemoteBulkWriter, BulkFileType


class MilvusUploader:
    def __init__(self, milvus_host="192.168.31.132", milvus_port="19530"):
        """
        åˆå§‹åŒ–Milvusè¿æ¥

        Args:
            milvus_host (str): MilvusæœåŠ¡å™¨åœ°å€
            milvus_port (str): MilvusæœåŠ¡å™¨ç«¯å£
        """
        self.milvus_host = milvus_host
        self.milvus_port = milvus_port
        self._connect_to_milvus()

    def _connect_to_milvus(self):
        """è¿æ¥åˆ°MilvusæœåŠ¡å™¨"""
        try:
            connections.connect("default", host=self.milvus_host, port=self.milvus_port)
            print(f"âœ… æˆåŠŸè¿æ¥åˆ°MilvusæœåŠ¡å™¨ {self.milvus_host}:{self.milvus_port}")
        except Exception as e:
            print(f"âŒ è¿æ¥Milvuså¤±è´¥: {str(e)}")
            raise

    def create_schema(self):
        """
        åˆ›å»ºè¡¨æ ¼æ•°æ®çš„Milvus Schema

        Returns:
            CollectionSchema: Milvusé›†åˆæ¨¡å¼
        """
        # å®šä¹‰å­—æ®µ
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="row_index", dtype=DataType.INT64, description="è¡Œç´¢å¼•"),
            FieldSchema(name="col_index", dtype=DataType.INT64, description="åˆ—ç´¢å¼•"),
            FieldSchema(name="col_name", dtype=DataType.VARCHAR, max_length=256, description="åˆ—å"),
            FieldSchema(name="cell_content", dtype=DataType.VARCHAR, max_length=65535, description="å•å…ƒæ ¼å†…å®¹"),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128, description="å†…å®¹å‘é‡è¡¨ç¤º")
        ]

        schema = CollectionSchema(
            fields=fields,
            description="OCRè¯†åˆ«çš„è¡¨æ ¼æ•°æ®"
        )
        return schema

    def create_or_get_collection(self, collection_name):
        """
        åˆ›å»ºæˆ–è·å–Milvusé›†åˆ

        Args:
            collection_name (str): é›†åˆåç§°

        Returns:
            Collection: Milvusé›†åˆå¯¹è±¡
        """
        try:
            schema = self.create_schema()

            # å¦‚æœé›†åˆå·²å­˜åœ¨ï¼ŒåŠ è½½å®ƒ
            if utility.has_collection(collection_name):
                collection = Collection(name=collection_name)
                print(f"ğŸ“¥ å·²åŠ è½½ç°æœ‰é›†åˆ: {collection_name}")
            else:
                # åˆ›å»ºæ–°é›†åˆ
                collection = Collection(name=collection_name, schema=schema)
                print(f"ğŸ†• åˆ›å»ºæ–°é›†åˆ: {collection_name}")

                # åˆ›å»ºç´¢å¼•
                index_params = {
                    "index_type": "IVF_FLAT",
                    "metric_type": "L2",
                    "params": {"nlist": 128}
                }

                collection.create_index(field_name="embedding", index_params=index_params)
                print(f".CreateIndex: åœ¨embeddingå­—æ®µä¸Šåˆ›å»ºç´¢å¼•")

            return collection

        except Exception as e:
            print(f"âŒ é›†åˆæ“ä½œå¤±è´¥: {str(e)}")
            raise

    def text_to_vector(self, text, dim=128):
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼ˆç®€å•å®ç°ï¼‰

        Args:
            text (str): è¾“å…¥æ–‡æœ¬
            dim (int): å‘é‡ç»´åº¦

        Returns:
            list: å‘é‡è¡¨ç¤º
        """
        if not text or not str(text).strip():
            return [0.0] * dim

        # ä½¿ç”¨ç®€å•å“ˆå¸Œæ–¹æ³•ç”Ÿæˆå‘é‡ï¼ˆä»…ä½œç¤ºä¾‹ï¼‰
        np.random.seed(hash(str(text)) % (2 ** 32))
        vector = np.random.random(dim).tolist()
        # å½’ä¸€åŒ–å‘é‡
        norm = np.linalg.norm(vector)
        if norm > 0:
            vector = [x / norm for x in vector]
        return vector

    def create_bulk_writer(self, collection_name, s3_config=None):
        """
        åˆ›å»ºRemoteBulkWriterç”¨äºæ‰¹é‡ä¸Šä¼ æ•°æ®

        Args:
            collection_name (str): é›†åˆåç§°
            s3_config (dict): S3é…ç½®å‚æ•°

        Returns:
            RemoteBulkWriter: æ‰¹é‡å†™å…¥å™¨
        """
        try:
            # é»˜è®¤S3é…ç½® - æ ¹æ®æ‚¨çš„æœåŠ¡å™¨IPé…ç½®
            if s3_config is None:
                s3_config = {
                    "endpoint": "192.168.31.132:9000",  # MinIOæœåŠ¡åœ°å€
                    "access_key": "minioadmin",  # MinIOè®¿é—®å¯†é’¥
                    "secret_key": "minioadmin",  # MinIOç§˜å¯†å¯†é’¥
                    "bucket_name": "a-bucket",  # å­˜å‚¨æ¡¶åç§°ï¼ˆéœ€è¦å…ˆåˆ›å»ºï¼‰
                    "secure": False  # æœ¬åœ°éƒ¨ç½²ï¼Œä¸ä½¿ç”¨HTTPS
                }

            # åˆ›å»ºé›†åˆSchema
            schema = self.create_schema()

            # S3è¿æ¥å‚æ•°
            conn = RemoteBulkWriter.S3ConnectParam(
                endpoint=s3_config["endpoint"],
                access_key=s3_config["access_key"],
                secret_key=s3_config["secret_key"],
                bucket_name=s3_config["bucket_name"],
                secure=s3_config["secure"]
            )

            # åˆ›å»ºBulkWriter
            writer = RemoteBulkWriter(
                schema=schema,
                remote_path=f"/{collection_name}/",
                connect_param=conn,
                file_type=BulkFileType.PARQUET
            )

            print(f"âœ… BulkWriteråˆ›å»ºæˆåŠŸï¼Œç›®æ ‡è·¯å¾„: /{collection_name}/")
            return writer

        except Exception as e:
            print(f"âŒ åˆ›å»ºBulkWriterå¤±è´¥: {str(e)}")
            raise

    def load_csv_data(self, csv_path):
        """
        åŠ è½½CSVæ–‡ä»¶æ•°æ®

        Args:
            csv_path (str): CSVæ–‡ä»¶è·¯å¾„

        Returns:
            pandas.DataFrame: åŠ è½½çš„æ•°æ®
        """
        try:
            if not os.path.exists(csv_path):
                raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")

            df = pd.read_csv(csv_path, encoding="utf-8")
            print(f"ğŸ“„ æˆåŠŸè¯»å–CSVæ–‡ä»¶: {csv_path}")
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape[0]}è¡Œ x {df.shape[1]}åˆ—")
            return df

        except Exception as e:
            print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
            raise

    def prepare_data_for_upload(self, df, vector_dim=128):
        """
        å‡†å¤‡ä¸Šä¼ åˆ°Milvusçš„æ•°æ®

        Args:
            df (pandas.DataFrame): è¡¨æ ¼æ•°æ®
            vector_dim (int): å‘é‡ç»´åº¦

        Returns:
            dict: å‡†å¤‡å¥½çš„æ•°æ®å­—å…¸
        """
        try:
            row_indices = []
            col_indices = []
            col_names = []
            cell_contents = []
            embeddings = []

            # éå†æ¯ä¸€è¡Œæ•°æ®
            for row_idx, row in df.iterrows():
                # éå†æ¯ä¸€åˆ—
                for col_idx, (col_name, cell_value) in enumerate(row.items()):
                    cell_value_str = str(cell_value) if not pd.isna(cell_value) else ""

                    row_indices.append(row_idx)
                    col_indices.append(col_idx)
                    col_names.append(str(col_name)[:255])  # é™åˆ¶åˆ—åé•¿åº¦
                    cell_contents.append(cell_value_str)
                    embeddings.append(self.text_to_vector(cell_value_str, vector_dim))

            data = {
                "row_index": row_indices,
                "col_index": col_indices,
                "col_name": col_names,
                "cell_content": cell_contents,
                "embedding": embeddings
            }

            print(f"âœ… å‡†å¤‡å¥½{len(row_indices)}æ¡è®°å½•ç”¨äºä¸Šä¼ ")
            return data

        except Exception as e:
            print(f"âŒ å‡†å¤‡ä¸Šä¼ æ•°æ®å¤±è´¥: {str(e)}")
            raise

    def upload_csv_to_milvus(self, csv_path, collection_name, s3_config=None, batch_size=1000):
        """
        å°†CSVæ–‡ä»¶ä¸Šä¼ åˆ°Milvus

        Args:
            csv_path (str): CSVæ–‡ä»¶è·¯å¾„
            collection_name (str): Milvusé›†åˆåç§°
            s3_config (dict): S3é…ç½®å‚æ•°
            batch_size (int): æ‰¹å¤„ç†å¤§å°
        """
        try:
            print(f"\nğŸ“¤ å¼€å§‹ä¸Šä¼ CSVæ–‡ä»¶åˆ°Milvus...")
            print(f"ğŸ“ CSVæ–‡ä»¶: {csv_path}")
            print(f"ğŸ“¦ é›†åˆåç§°: {collection_name}")

            # åˆ›å»ºBulkWriter
            writer = self.create_bulk_writer(collection_name, s3_config)

            # åŠ è½½æ•°æ®
            df = self.load_csv_data(csv_path)

            # å‡†å¤‡æ•°æ®
            data = self.prepare_data_for_upload(df)

            # åˆ†æ‰¹å†™å…¥æ•°æ®
            total_records = len(data["row_index"])
            uploaded_records = 0

            for i in range(0, total_records, batch_size):
                batch_end = min(i + batch_size, total_records)

                batch_data = {}
                for key, value_list in data.items():
                    batch_data[key] = value_list[i:batch_end]

                writer.append_row(batch_data)
                uploaded_records += len(batch_data["row_index"])
                print(f"ğŸ“ˆ å·²å¤„ç†: {uploaded_records}/{total_records} æ¡è®°å½•")

            # æäº¤æ•°æ®
            writer.commit()
            print(f"âœ… æˆåŠŸä¸Šä¼ {uploaded_records}æ¡è®°å½•åˆ°Milvusé›†åˆ '{collection_name}'")

            # åŠ è½½é›†åˆ
            collection = self.create_or_get_collection(collection_name)
            collection.load()
            print(f"ğŸ”„ é›†åˆ '{collection_name}' å·²åŠ è½½åˆ°å†…å­˜")

        except Exception as e:
            print(f"âŒ ä¸Šä¼ æ•°æ®åˆ°Milvuså¤±è´¥: {str(e)}")
            raise

    def query_collection_info(self, collection_name):
        """
        æŸ¥è¯¢é›†åˆä¿¡æ¯

        Args:
            collection_name (str): é›†åˆåç§°
        """
        try:
            if utility.has_collection(collection_name):
                collection = Collection(name=collection_name)
                print(f"\nğŸ” é›†åˆ '{collection_name}' ä¿¡æ¯:")
                print(f"   è¡Œæ•°: {collection.num_entities}")
                print(f"   å­—æ®µ: {[field.name for field in collection.schema.fields]}")
            else:
                print(f"âš ï¸  é›†åˆ '{collection_name}' ä¸å­˜åœ¨")
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢é›†åˆä¿¡æ¯å¤±è´¥: {str(e)}")


def main():
    """ä¸»å‡½æ•° - ä¸Šä¼ CSVåˆ°Milvus"""
    print("=" * 60)
    print("ğŸ“¦ CSVæ–‡ä»¶ä¸Šä¼ åˆ°Milvuså·¥å…·")
    print("=" * 60)

    # é…ç½®å‚æ•°
    CSV_FILE_PATH = "simple_reconstructed_table.csv"  # CSVæ–‡ä»¶è·¯å¾„
    COLLECTION_NAME = "ocr_table_data"  # Milvusé›†åˆåç§°

    # MilvusæœåŠ¡å™¨é…ç½®
    MILVUS_HOST = "192.168.31.132"  # MilvusæœåŠ¡å™¨IPåœ°å€
    MILVUS_PORT = "19530"  # MilvusæœåŠ¡ç«¯å£

    # S3é…ç½®ï¼ˆMinIOé…ç½®ï¼‰
    S3_CONFIG = {
        "endpoint": "192.168.31.132:9000",  # MinIOæœåŠ¡åœ°å€
        "access_key": "minioadmin",  # MinIOè®¿é—®å¯†é’¥
        "secret_key": "minioadmin",  # MinIOç§˜å¯†å¯†é’¥
        "bucket_name": "a-bucket",  # éœ€è¦å…ˆåœ¨MinIOä¸­åˆ›å»ºæ­¤å­˜å‚¨æ¡¶
        "secure": False  # æœ¬åœ°éƒ¨ç½²ï¼Œä¸ä½¿ç”¨HTTPS
    }

    try:
        # åˆ›å»ºä¸Šä¼ å™¨å®ä¾‹
        uploader = MilvusUploader(milvus_host=MILVUS_HOST, milvus_port=MILVUS_PORT)

        # ä¸Šä¼ CSVåˆ°Milvus
        uploader.upload_csv_to_milvus(
            csv_path=CSV_FILE_PATH,
            collection_name=COLLECTION_NAME,
            s3_config=S3_CONFIG,
            batch_size=1000
        )

        # æ˜¾ç¤ºé›†åˆä¿¡æ¯
        uploader.query_collection_info(COLLECTION_NAME)

        print("\nâœ… æ•°æ®ä¸Šä¼ å®Œæˆ!")

    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        return


if __name__ == "__main__":
    main()
