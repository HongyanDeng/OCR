# upload_to_milvus.py
import pandas as pd
import numpy as np
import os
from pymilvus import (
    connections,
    CollectionSchema,
    FieldSchema,
    DataType,
    utility,
    Collection
)
from pymilvus.bulk_writer import RemoteBulkWriter, BulkFileType


class MilvusUploader:
    def __init__(self, milvus_host="192.168.31.132", milvus_port="19530"):
        """
        åˆå§‹åŒ–Milvusè¿æ¥

        Args:
            milvus_host (str): MilvusæœåŠ¡å™¨åœ°å€
            milvus_port (str): MilvusæœåŠ¡å™¨ç«¯å£
        """
        self.milvus_host = milvus_host
        self.milvus_port = milvus_port
        self._connect_to_milvus()

    def _connect_to_milvus(self):
        """è¿æ¥åˆ°MilvusæœåŠ¡å™¨"""
        try:
            connections.connect("default", host=self.milvus_host, port=self.milvus_port)
            print(f"âœ… æˆåŠŸè¿æ¥åˆ°MilvusæœåŠ¡å™¨ {self.milvus_host}:{self.milvus_port}")
        except Exception as e:
            print(f"âŒ è¿æ¥Milvuså¤±è´¥: {str(e)}")
            raise

    def create_schema(self):
        """
        åˆ›å»ºè¡¨æ ¼æ•°æ®çš„Milvus Schema

        Returns:
            CollectionSchema: Milvusé›†åˆæ¨¡å¼
        """
        # å®šä¹‰å­—æ®µ - ä½¿ç”¨æœ€ç®€å•çš„æ•°æ®ç±»å‹
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="row_index", dtype=DataType.INT64, description="è¡Œç´¢å¼•"),
            FieldSchema(name="col_index", dtype=DataType.INT64, description="åˆ—ç´¢å¼•"),
            FieldSchema(name="col_name", dtype=DataType.VARCHAR, max_length=256, description="åˆ—å"),
            FieldSchema(name="cell_content", dtype=DataType.VARCHAR, max_length=65535, description="å•å…ƒæ ¼å†…å®¹"),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=128, description="å†…å®¹å‘é‡è¡¨ç¤º")  # è¿›ä¸€æ­¥å‡å°å‘é‡ç»´åº¦
        ]

        schema = CollectionSchema(
            fields=fields,
            description="OCRè¯†åˆ«çš„è¡¨æ ¼æ•°æ®"
        )
        return schema

    def create_or_get_collection(self, collection_name):
        """
        åˆ›å»ºæˆ–è·å–Milvusé›†åˆ

        Args:
            collection_name (str): é›†åˆåç§°

        Returns:
            Collection: Milvusé›†åˆå¯¹è±¡
        """
        try:
            schema = self.create_schema()

            # å¦‚æœé›†åˆå·²å­˜åœ¨ï¼Œåˆ é™¤å®ƒå¹¶é‡æ–°åˆ›å»º
            if utility.has_collection(collection_name):
                utility.drop_collection(collection_name)
                print(f"ğŸ—‘ï¸ å·²åˆ é™¤ç°æœ‰é›†åˆ: {collection_name}")

            # åˆ›å»ºæ–°é›†åˆ
            collection = Collection(name=collection_name, schema=schema)
            print(f"ğŸ†• åˆ›å»ºæ–°é›†åˆ: {collection_name}")

            # åˆ›å»ºç´¢å¼•
            index_params = {
                "index_type": "FLAT",
                "metric_type": "L2",
                "params": {}
            }

            collection.create_index(field_name="embedding", index_params=index_params)
            print(f".CreateIndex: åœ¨embeddingå­—æ®µä¸Šåˆ›å»ºç´¢å¼•")

            return collection

        except Exception as e:
            print(f"âŒ é›†åˆæ“ä½œå¤±è´¥: {str(e)}")
            raise

    def text_to_vector(self, text, dim=128):
        """
        å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡è¡¨ç¤ºï¼ˆç®€å•å®ç°ï¼‰

        Args:
            text (str): è¾“å…¥æ–‡æœ¬
            dim (int): å‘é‡ç»´åº¦

        Returns:
            list: å‘é‡è¡¨ç¤º
        """
        # æœ€ç®€å•å¯é çš„å‘é‡ç”Ÿæˆæ–¹æ³•
        if not text or not str(text).strip():
            return [0.1] * dim

        # ä½¿ç”¨hashå€¼ç”Ÿæˆå›ºå®šå‘é‡
        hash_val = hash(str(text))
        np.random.seed(abs(hash_val) % (2 ** 32))
        vector = np.random.random(dim).tolist()

        # å½’ä¸€åŒ–
        magnitude = sum(x * x for x in vector) ** 0.5
        if magnitude > 0:
            vector = [x / magnitude for x in vector]

        return vector

    def create_bulk_writer(self, collection_name, s3_config=None):
        """
        åˆ›å»ºRemoteBulkWriterç”¨äºæ‰¹é‡ä¸Šä¼ æ•°æ®

        Args:
            collection_name (str): é›†åˆåç§°
            s3_config (dict): S3é…ç½®å‚æ•°

        Returns:
            RemoteBulkWriter: æ‰¹é‡å†™å…¥å™¨
        """
        try:
            # é»˜è®¤S3é…ç½® - æ ¹æ®æ‚¨çš„æœåŠ¡å™¨IPé…ç½®
            if s3_config is None:
                s3_config = {
                    "endpoint": "192.168.31.132:9000",  # MinIOæœåŠ¡åœ°å€
                    "access_key": "minioadmin",  # MinIOè®¿é—®å¯†é’¥
                    "secret_key": "minioadmin",  # MinIOç§˜å¯†å¯†é’¥
                    "bucket_name": "a-bucket",  # å­˜å‚¨æ¡¶åç§°ï¼ˆéœ€è¦å…ˆåˆ›å»ºï¼‰
                    "secure": False  # æœ¬åœ°éƒ¨ç½²ï¼Œä¸ä½¿ç”¨HTTPS
                }

            # åˆ›å»ºé›†åˆSchema
            schema = self.create_schema()

            # S3è¿æ¥å‚æ•°
            conn = RemoteBulkWriter.S3ConnectParam(
                endpoint=s3_config["endpoint"],
                access_key=s3_config["access_key"],
                secret_key=s3_config["secret_key"],
                bucket_name=s3_config["bucket_name"],
                secure=s3_config["secure"]
            )

            # åˆ›å»ºBulkWriter
            writer = RemoteBulkWriter(
                schema=schema,
                remote_path=f"/{collection_name}/",
                connect_param=conn,
                file_type=BulkFileType.PARQUET
            )

            print(f"âœ… BulkWriteråˆ›å»ºæˆåŠŸï¼Œç›®æ ‡è·¯å¾„: /{collection_name}/")
            return writer

        except Exception as e:
            print(f"âŒ åˆ›å»ºBulkWriterå¤±è´¥: {str(e)}")
            raise

    def load_csv_data(self, csv_path):
        """
        åŠ è½½CSVæ–‡ä»¶æ•°æ®

        Args:
            csv_path (str): CSVæ–‡ä»¶è·¯å¾„

        Returns:
            pandas.DataFrame: åŠ è½½çš„æ•°æ®
        """
        try:
            if not os.path.exists(csv_path):
                raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")

            # ä½¿ç”¨æœ€ç®€å•çš„å‚æ•°è¯»å–CSV
            df = pd.read_csv(csv_path, encoding="utf-8", dtype=object, keep_default_na=False, na_values=[])
            print(f"ğŸ“„ æˆåŠŸè¯»å–CSVæ–‡ä»¶: {csv_path}")
            print(f"ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape[0]}è¡Œ x {df.shape[1]}åˆ—")

            return df

        except Exception as e:
            print(f"âŒ è¯»å–CSVæ–‡ä»¶å¤±è´¥: {str(e)}")
            raise

    def prepare_data_for_upload(self, df, vector_dim=128):
        """
        å‡†å¤‡ä¸Šä¼ åˆ°Milvusçš„æ•°æ®

        Args:
            df (pandas.DataFrame): è¡¨æ ¼æ•°æ®
            vector_dim (int): å‘é‡ç»´åº¦

        Returns:
            dict: å‡†å¤‡å¥½çš„æ•°æ®å­—å…¸
        """
        try:
            row_indices = []
            col_indices = []
            col_names = []
            cell_contents = []
            embeddings = []

            print(f"ğŸ”„ å¼€å§‹å¤„ç†æ•°æ®ï¼Œå…±{len(df)}è¡Œï¼Œ{len(df.columns)}åˆ—")

            # éå†æ¯ä¸€è¡Œæ•°æ®
            for row_idx in range(len(df)):
                row = df.iloc[row_idx]
                # éå†æ¯ä¸€åˆ—
                for col_idx in range(len(df.columns)):
                    col_name = df.columns[col_idx]
                    cell_value = row[col_idx]

                    # ç¡®ä¿å€¼æ˜¯å­—ç¬¦ä¸²
                    cell_value_str = str(cell_value) if cell_value is not None and str(cell_value) != 'nan' else ""
                    col_name_str = str(col_name) if col_name is not None else f"col_{col_idx}"

                    # æ·»åŠ æ•°æ® - ç¡®ä¿ä½¿ç”¨PythonåŸç”Ÿintç±»å‹
                    row_indices.append(int(row_idx))
                    col_indices.append(int(col_idx))
                    col_names.append(col_name_str[:255])  # é™åˆ¶é•¿åº¦
                    cell_contents.append(cell_value_str)
                    embeddings.append(self.text_to_vector(cell_value_str, vector_dim))

                    # è°ƒè¯•ä¿¡æ¯ï¼ˆä»…æ˜¾ç¤ºå‰å‡ æ¡ï¼‰
                    if len(row_indices) <= 3:
                        print(
                            f"ğŸ“ è®°å½• {len(row_indices)}: è¡Œ={row_idx}, åˆ—={col_idx}, åˆ—å={col_name_str}, å†…å®¹='{cell_value_str[:30]}...'")

            data = {
                "row_index": [int(x) for x in row_indices],  # ç¡®ä¿æ˜¯Python int
                "col_index": [int(x) for x in col_indices],  # ç¡®ä¿æ˜¯Python int
                "col_name": col_names,
                "cell_content": cell_contents,
                "embedding": embeddings
            }

            print(f"âœ… å‡†å¤‡å¥½{len(row_indices)}æ¡è®°å½•ç”¨äºä¸Šä¼ ")
            return data

        except Exception as e:
            print(f"âŒ å‡†å¤‡ä¸Šä¼ æ•°æ®å¤±è´¥: {str(e)}")
            raise

    def upload_csv_to_milvus(self, csv_path, collection_name, s3_config=None, batch_size=5):
        """
        å°†CSVæ–‡ä»¶ä¸Šä¼ åˆ°Milvus

        Args:
            csv_path (str): CSVæ–‡ä»¶è·¯å¾„
            collection_name (str): Milvusé›†åˆåç§°
            s3_config (dict): S3é…ç½®å‚æ•°
            batch_size (int): æ‰¹å¤„ç†å¤§å°
        """
        try:
            print(f"\nğŸ“¤ å¼€å§‹ä¸Šä¼ CSVæ–‡ä»¶åˆ°Milvus...")
            print(f"ğŸ“ CSVæ–‡ä»¶: {csv_path}")
            print(f"ğŸ“¦ é›†åˆåç§°: {collection_name}")

            # åˆ›å»ºBulkWriter
            writer = self.create_bulk_writer(collection_name, s3_config)

            # åŠ è½½æ•°æ®
            df = self.load_csv_data(csv_path)

            # å‡†å¤‡æ•°æ®
            data = self.prepare_data_for_upload(df, vector_dim=128)  # ä½¿ç”¨æ›´å°çš„å‘é‡ç»´åº¦

            # åˆ†æ‰¹å†™å…¥æ•°æ®
            total_records = len(data["row_index"])
            uploaded_records = 0
            successful_batches = 0

            for i in range(0, total_records, batch_size):
                batch_end = min(i + batch_size, total_records)



                # æ„å»ºæ‰¹æ¬¡æ•°æ®
                batch_data = {}
                for key in ["row_index", "col_index", "col_name", "cell_content", "embedding"]:
                    batch_data[key] = data[key][i:batch_end]

                # ç¡®ä¿æ•°æ®ç±»å‹å®Œå…¨æ­£ç¡®å¹¶æ‰“å°è°ƒè¯•ä¿¡æ¯
                final_batch_data = {}
                print(f"\nğŸ” éªŒè¯æ‰¹æ¬¡ {i // batch_size + 1} æ•°æ®:")


                for key, value_list in batch_data.items():
                    if key in ["row_index", "col_index"]:
                        processed_list = []
                        for x in value_list:
                            try:
                                processed_list.append(int(float(str(x))))
                            except:
                                processed_list.append(np.int64(0))
                        final_batch_data[key] = processed_list
                        print(f"  {key}: {processed_list[:5]}")
                        print(f"  {key} types: {[type(x) for x in processed_list[:5]]}")  # æ‰“å°ç±»å‹
                    elif key in ["col_name", "cell_content"]:
                        # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
                        processed_list = [str(x) if x is not None else "" for x in value_list]
                        final_batch_data[key] = processed_list
                        print(f"  {key}: {[x[:20] for x in processed_list[:5]]}")
                    elif key == "embedding":
                        # ç¡®ä¿æ˜¯æµ®ç‚¹æ•°å‘é‡åˆ—è¡¨
                        validated_embeddings = []
                        for emb in value_list:
                            if isinstance(emb, (list, tuple)) and len(emb) == 128 :
                                validated_embeddings.append([float(x) for x in emb])
                            else:
                                # ä½¿ç”¨é»˜è®¤å‘é‡
                                validated_embeddings.append([0.1] * 128)
                        final_batch_data[key] = validated_embeddings
                        print(f"  {key}: é•¿åº¦={[len(x) for x in validated_embeddings[:3]]}")
                    else:
                        final_batch_data[key] = list(value_list)

                # æ‰“å°æ•´ä¸ª final_batch_data çš„ç»“æ„
                print(f"  final_batch_data: {final_batch_data}")
                # ===== æ·»åŠ çš„ç±»å‹æ£€æŸ¥ä»£ç  =====
                print(f"æ•°æ®ç±»å‹æ£€æŸ¥ - row_index: {[type(x) for x in final_batch_data['row_index'][:5]]}")
                print(f"æ•°æ®ç±»å‹æ£€æŸ¥ - col_index: {[type(x) for x in final_batch_data['col_index'][:5]]}")

                try:
                    writer.append_row(final_batch_data)
                    batch_record_count = len(final_batch_data["row_index"])
                    uploaded_records += batch_record_count
                    successful_batches += 1
                    print(
                        f"âœ… æˆåŠŸå¤„ç†æ‰¹æ¬¡ {i // batch_size + 1}: {batch_record_count} æ¡è®°å½• (ç´¯è®¡: {uploaded_records}/{total_records})")
                except Exception as e:
                    print(f"âŒ æ‰¹æ¬¡å†™å…¥å¤±è´¥ (è®°å½• {i + 1}-{batch_end}): {str(e)}")

            # æäº¤æ•°æ®
            try:
                writer.commit()
                print(f"âœ… æˆåŠŸæäº¤æ•°æ®ï¼Œå…±ä¸Šä¼ {uploaded_records}æ¡è®°å½•åˆ°Milvusé›†åˆ '{collection_name}'")
                print(f"   æˆåŠŸæ‰¹æ¬¡æ•°é‡: {successful_batches}")
            except Exception as e:
                print(f"âš ï¸  æäº¤æ•°æ®æ—¶å‡ºé”™: {str(e)}")

            # åŠ è½½é›†åˆ
            collection = self.create_or_get_collection(collection_name)
            collection.load()
            print(f"ğŸ”„ é›†åˆ '{collection_name}' å·²åŠ è½½åˆ°å†…å­˜")

        except Exception as e:
            print(f"âŒ ä¸Šä¼ æ•°æ®åˆ°Milvuså¤±è´¥: {str(e)}")
            raise

    def query_collection_info(self, collection_name):
        """
        æŸ¥è¯¢é›†åˆä¿¡æ¯

        Args:
            collection_name (str): é›†åˆåç§°
        """
        try:
            if utility.has_collection(collection_name):
                collection = Collection(name=collection_name)
                print(f"\nğŸ” é›†åˆ '{collection_name}' ä¿¡æ¯:")
                print(f"   è¡Œæ•°: {collection.num_entities}")
                print(f"   å­—æ®µ: {[field.name for field in collection.schema.fields]}")

                # æ˜¾ç¤ºéƒ¨åˆ†æ•°æ®
                if collection.num_entities > 0:
                    try:
                        collection.load()
                        results = collection.query(expr="row_index < 3",
                                                   output_fields=["row_index", "col_index", "col_name", "cell_content"],
                                                   limit=10)
                        print(f"   ç¤ºä¾‹æ•°æ®:")
                        for i, result in enumerate(results):
                            print(
                                f"     {i + 1}. è¡Œ:{result.get('row_index', 'N/A')}, åˆ—:{result.get('col_index', 'N/A')}, "
                                f"åˆ—å:'{result.get('col_name', 'N/A')[:20]}...', å†…å®¹:'{result.get('cell_content', 'N/A')[:30]}...'")
                    except Exception as e:
                        print(f"   æ— æ³•æŸ¥è¯¢ç¤ºä¾‹æ•°æ®: {str(e)}")
            else:
                print(f"âš ï¸  é›†åˆ '{collection_name}' ä¸å­˜åœ¨")
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢é›†åˆä¿¡æ¯å¤±è´¥: {str(e)}")


def main():
    """ä¸»å‡½æ•° - ä¸Šä¼ CSVåˆ°Milvus"""
    print("=" * 60)
    print("ğŸ“¦ CSVæ–‡ä»¶ä¸Šä¼ åˆ°Milvuså·¥å…·")
    print("=" * 60)

    # é…ç½®å‚æ•°
    CSV_FILE_PATH = "simple_reconstructed_table.csv"  # CSVæ–‡ä»¶è·¯å¾„
    COLLECTION_NAME = "ocr_table_data"  # Milvusé›†åˆåç§°

    # MilvusæœåŠ¡å™¨é…ç½®
    MILVUS_HOST = "192.168.31.132"  # MilvusæœåŠ¡å™¨IPåœ°å€
    MILVUS_PORT = "19530"  # MilvusæœåŠ¡ç«¯å£

    # S3é…ç½®ï¼ˆMinIOé…ç½®ï¼‰
    S3_CONFIG = {
        "endpoint": "192.168.31.132:9000",  # MinIOæœåŠ¡åœ°å€
        "access_key": "minioadmin",  # MinIOè®¿é—®å¯†é’¥
        "secret_key": "minioadmin",  # MinIOç§˜å¯†å¯†é’¥
        "bucket_name": "a-bucket",  # éœ€è¦å…ˆåœ¨MinIOä¸­åˆ›å»ºæ­¤å­˜å‚¨æ¡¶
        "secure": False  # æœ¬åœ°éƒ¨ç½²ï¼Œä¸ä½¿ç”¨HTTPS
    }

    try:
        # åˆ›å»ºä¸Šä¼ å™¨å®ä¾‹
        uploader = MilvusUploader(milvus_host=MILVUS_HOST, milvus_port=MILVUS_PORT)

        # ä¸Šä¼ CSVåˆ°Milvus
        uploader.upload_csv_to_milvus(
            csv_path=CSV_FILE_PATH,
            collection_name=COLLECTION_NAME,
            s3_config=S3_CONFIG,
            batch_size=5  # ä½¿ç”¨æœ€å°çš„æ‰¹æ¬¡å¤§å°
        )

        # æ˜¾ç¤ºé›†åˆä¿¡æ¯
        uploader.query_collection_info(COLLECTION_NAME)

        print("\nâœ… æ•°æ®ä¸Šä¼ å®Œæˆ!")

    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        return


if __name__ == "__main__":
    main()
